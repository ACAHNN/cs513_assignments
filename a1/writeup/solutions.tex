\subsection{Question 1}

\subsubsection{Question 1.1}
\subsubsection{Part A}

\begin{enumerate}

\item double column 1 (postmultiply):
  \begin{equation}
    X\{1\} = 
    \begin{pmatrix}
      2 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1
    \end{pmatrix}
  \end{equation}

\item halve row 3 (premultiply):

  \begin{equation}
    X\{2\} = 
    \begin{pmatrix}
      1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 0.5 & 0 \\
      0 & 0 & 0 & 1
    \end{pmatrix}
  \end{equation}

\item add row 3 to row 1 (premultiply):

  \begin{equation}
    X\{3\} = 
    \begin{pmatrix}
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1
    \end{pmatrix}
  \end{equation}

\item interchange columns 1 and 4 (postmultiply):

  \begin{equation}
    X\{4\} = 
    \begin{pmatrix}
      0 & 0 & 0 & 1 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      1 & 0 & 0 & 0
    \end{pmatrix}
  \end{equation}

\item subtract row 2 from each of the other rows (premultiply):

  \begin{equation}
    X\{5\} = 
    \begin{pmatrix}
      1 & -1 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & -1 & 1 & 0 \\
      0 & -1 & 0 & 1
    \end{pmatrix}
  \end{equation}

\item replace column 4 by column 3 (postmultiply):

  \begin{equation}
    X\{6\} = 
    \begin{pmatrix}
      1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 1 \\
      0 & 0 & 0 & 0
    \end{pmatrix}
  \end{equation}

\item delete column 1 (postmultiply):

  \begin{equation}
    X\{7\} = 
    \begin{pmatrix}
      0 & 0 & 0 \\
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{pmatrix}
  \end{equation}

\end{enumerate}

The result as a product of eight matrices is as follows:

\begin{equation}
  \boxed{M = X\{5\}*X\{3\}*X\{2\}*B*X\{1\}*X\{4\}*X\{6\}*X\{7\}}
  \label{eq:1a}
\end{equation}

\newpage
\subsubsection{Part B}

(\ref{eq:1a}) can be rewritten as a product of three matrices (same B) as follows:

\begin{equation}
  A = X\{5\}*X\{3\}*X\{2\}
\end{equation}

\begin{equation}
  A = 
  \begin{pmatrix}
    1 & -1 & 0.5 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & -1 & 0.5 & 0\\
    0 & -1 & 0 & 1
  \end{pmatrix}
\end{equation}


\begin{equation}
  C = X\{1\}*X\{4\}*X\{6\}*X\{7\}
\end{equation}

\begin{equation}
  C = 
  \begin{pmatrix}
    0 & 0 & 0 \\
    1 & 0 & 0 \\
    0 & 1 & 1 \\
    0 & 0 & 0
  \end{pmatrix}
\end{equation}


This results in:

\begin{equation}
  \boxed{M = A*B*C}
\end{equation}

\newpage
\subsubsection{Matlab Code for Parts A \& B}
\lstinputlisting[caption=Matlab Commands,showstringspaces=false,language=Matlab]{../a1_q1_1.m}

\newpage
\subsubsection{Question 2.3}

Let \(A \in \mathbbm{C}^{m \times m} \) be hermitian.
By definition \(A = A^{*} \).

\subsubsection{Part A}

Prove that all eigenvalues of \(A\) are real.
Assume \( \lambda \) is an eigenvalue of \(A\) and \(v\) is eigenvector associated with \( \lambda \).
The proof is as follows:

\begin{equation}
  \lambda \langle v,v \rangle 
  = \langle \lambda v,v \rangle 
  = \langle Av,v \rangle
  = \langle v,A^{*}v \rangle
  = \langle v,Av \rangle
  = \langle v,\lambda v \rangle
  = \bar{\lambda} \langle v,v \rangle
\end{equation}

\begin{equation}
  \lambda \langle v,v \rangle
  = \bar{\lambda} \langle v,v \rangle
\label{eq:2.1}
\end{equation}

Positive-definiteness says,

\begin{equation}
  \langle x,x \rangle \geq 0
\end{equation}
\begin{equation}
  \langle x,x \rangle = 0 \Rightarrow x = 0
\end{equation}

By definition eigenvectors cannot be the zero vector.
Therefore,

\begin{equation}
  \langle v,v \rangle \ne 0
\end{equation}

This means we can divide both sides of (\ref{eq:2.1}) by \( \langle v,v \rangle \).
This leaves,

\begin{equation}
  \lambda = \bar{\lambda}
\end{equation}

By definition, for a real scalar \(z\), \(\bar{z} = z\).
Therefore, \( \lambda \) is real valued and since we choose \(\lambda\) to be an arbitrary eigenvalue of \(A\), all eigenvalues of \(A\) are real.

\newpage
\subsubsection{Part B}

Prove that if \(x\) and \(y\) are eigenvectors corresponding to distinct eigenvalues, then \(x\) and \(y\) are orthogonal.
Note, a pair of vectors \(w_1\) and \(w_2\) are orthogonal if \( \langle w_1,w_2 \rangle = 0\).
We will therefore prove orthogonality of \(x\) and \(y\) by proving the equivalent statement: \( \langle x,y \rangle = 0 \).
The proof is as follows:
\\
\\
\indent Assume \(\lambda\) is the eigenvalue associated with \(x\), \(\mu\) is the eigenvalue associated with \(y\), and \(\lambda \ne \mu\).
Then,

\begin{equation}
  \lambda \langle x,y \rangle
  = \langle \lambda x,y \rangle
  = \langle Ax,y \rangle
  = \langle x,A^{*}y \rangle
  = \langle x,Ay \rangle
  = \langle x,\mu y \rangle
  = \bar{\mu} \langle x,y \rangle
\end{equation}

From the previous proof we know that \(\bar{\mu} = \mu\).
Therefore we are left with the following equation,

\begin{equation}
  \lambda \langle x,y \rangle = \mu \langle x,y \rangle
\end{equation}

We can then reorder terms,

\begin{equation}
  \lambda \langle x,y \rangle - \mu \langle x,y \rangle = 0
\end{equation}

This reduces to,

\begin{equation}
  (\lambda - \mu) \langle x,y \rangle = 0
\end{equation}

Allowing \( (\lambda -\mu) = 0\) would imply \( \lambda = \mu \).
However, this contradicts our assumption that \(\lambda\) and \(\mu\) are distinct; \( \lambda \ne \mu \).
Therefore, \( \langle x,y \rangle = 0 \).
This proves the equivalent statement that eigenvectors \(x\) and \(y\) are orthogonal.


\newpage
\subsection{Question 2}

Let \(Q\) by an \(m \times m\) real matrix that satisfies, for every vector \(x \in \mathbbm{R}^{m}\),

\begin{equation}
  ||Qx|| = ||x||
\end{equation}

\subsubsection{Part A}
\label{sec:q2_partA}
Show that 1 is the only eigenvalue of \(Q'Q\), i.e., show that \(\sigma(Q'Q) = \{1\}\).

To prove this we will show that \(Q'Q = I\), i.e., \(Q'Q\) is the identity matrix.
This is equivalent because 1 is the only eigenvalue of any indentity matrix.
To find the eigenvalues of \(I\) we must satisfy,

\begin{equation}
  \det(I - \lambda I)=0
\end{equation}

Since \(I\) is a diagonal matrix the matrix \(I - \lambda I\) will also be diagonal.
The diagonal entries will be of the form \(1 - \lambda\).
The determinant of \(I - \lambda I\) is the product of its diagonal entries.
Which means the characteristic polynomial is of the form,

\begin{equation}
  {(1 - \lambda)}_1 {(1 - \lambda)}_2 \ldots {(1 - \lambda)}_m = 0
\end{equation}

Therefore, the \(\lambda\)'s all equal 1 and these are the eigenvalues of \(I\).
Concisely,

\begin{equation}
  \boxed{p(I) = \{1\}}
  \label{eq:q2_a}
\end{equation}

Now we will prove \(Q'Q = I\).
We know that \(||x|| = \sqrt{\langle x,x \rangle}\).
We will assume \(x \ne 0\), as this is a degenerate case.
The proof is as follows,

\begin{eqnarray}
  ||Qx|| &=& ||x|| \\
  \sqrt{\langle Qx,Qx \rangle} &=& \sqrt{\langle x,x \rangle}
\end{eqnarray}

We can drop the square roots on both sides of the equation after the arrow and continue,

\begin{eqnarray}
  \langle Qx,Qx \rangle &=& \langle x,x \rangle \\
  x'Q'Qx &=& x'x
  \label{eq:q2_b}
\end{eqnarray} 

In order to satisfy (\ref{eq:q2_b}), \(Q'Q\) must equal the identity matrix, i.e., \(Q'Q = I\).
We have already shown the only eigenvalue of the identity matrix is 1.
Therefore, (\ref{eq:q2_a}) implies,

\begin{eqnarray}
  \boxed{p(Q'Q) = \{1\}}
\end{eqnarray}

This concludes the proof.

\newpage
\subsubsection{Part B}

We must first prove that \(Q'Q\) is symmetric for any matrix \(Q\).
A matrix is symmetric if \(A = A'\).
To do this we will define \(B = Q'Q\).
Then,

\begin{eqnarray}
  B' &=& (Q'Q)' \\
  &=& Q'Q'' \\
  &=& Q'Q \\
  &=& B
\end{eqnarray}

We have shown \(B = B'\) which is the definition of symmetric. 
Therefore, \(Q'Q\) is symmetric for any \(Q\). 

Using the previous proof, we must now show that \(Q\) from Section~\ref{sec:q2_partA} is orthogonal.
A matrix \(A\) is orthogonal if \(A' = A^{-1}\).

In Section~\ref{sec:q2_partA} we showed that \(Q'Q = I\).
The proof is as follows,

\begin{eqnarray}
  Q'Q &=& I = Q^{-1}Q \\
  Q'Q &=& Q^{-1}Q
\end{eqnarray}

Therefore, \(Q' = Q^{-1}\).
Since the transpose of \(Q\) is equal to the inverse of \(Q\), by definition \(Q\) is orthogonal.

\newpage
\subsection{Question 3}

The {\em sloppy\_qr.m} algorithm executes a loop \(n\) times.
Within the loop we have operations that have complexity \(n\), \(2n\), and \(2n^{3}\).
The complexity of the algorithm is,

\begin{eqnarray}
  n(2n^{3}+3n) \Rightarrow \boxed{2n^{4}+3n^2}
\end{eqnarray}

Therefore, the big-O complexity is \(2n^{4}\) or O(\(n^4\)).
\\
\\
The code for the experiments:
\lstinputlisting[caption=Matlab Commands,showstringspaces=false,language=Matlab]{../a1_q3.m}
